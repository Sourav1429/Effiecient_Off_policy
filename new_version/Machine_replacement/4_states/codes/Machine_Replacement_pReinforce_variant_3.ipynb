{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec4667bf",
   "metadata": {},
   "source": [
    "We begin by importing all the necssary packages and modules. Next our task is to build the Machine_Replacement environment. For doing that, we create a class named as Machine_Replacement which accepts the number_of_states(nS), number_of_actions(nA) and replacement_cost(rep_cost) as input and generate the environment. Later we just need to cal the function gen_probability() and gen_expected_reward_function() to get the Probability distribution matrix and Reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b39787b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn',True)\n",
    "torch.multiprocessing.set_start_method('spawn',True)\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Machine_Replacement:\n",
    "    def __init__(self,rep_cost=0.7,nS=6,nA=2):\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "        self.cost = np.linspace(0.1, 0.99,nS);\n",
    "        self.rep_cost = rep_cost;\n",
    "    def gen_probability(self):\n",
    "        self.P = np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            for j in range(self.nS):\n",
    "                if(i<=j):\n",
    "                    self.P[0,i,j]=(i+1)*(j+1);\n",
    "                else:\n",
    "                    continue;\n",
    "            self.P[0,i,:]=self.P[0,i,:]/np.sum(self.P[0,i,:])\n",
    "            self.P[1,i,0]=1;\n",
    "        return self.P;\n",
    "    def gen_reward(self):\n",
    "        self.R=np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i,:] = self.cost[i];\n",
    "            self.R[1,i,0] = self.rep_cost+self.cost[0];\n",
    "        return self.R;\n",
    "    def gen_expected_reward(self):\n",
    "        self.R = np.zeros((self.nA,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i] = self.cost[i];\n",
    "            self.R[1,i] = self.rep_cost + self.cost[0];\n",
    "        return self.R;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa2ee9e",
   "metadata": {},
   "source": [
    "Next we create another class specifying the hyperparameters that might be required in our algorithm. Later when required we can just call ret_hyperparameters() to get the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588bd079",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.T = 50000;\n",
    "        self.runs = 5;\n",
    "        self.lr = 0.1;\n",
    "        self.batch_size = 50;\n",
    "        self.start = 0;\n",
    "        self.nS = 4;\n",
    "        self.nA = 2;\n",
    "        self.rep_cost = 0.7\n",
    "        self.alpha = 0.2\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def ret_hyperparameters(self):\n",
    "        return (self.T,self.runs,self.lr,self.batch_size,self.start,self.nS,self.nA,self.rep_cost,self.alpha,self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f84ef0",
   "metadata": {},
   "source": [
    "Let us now define the pytorch model. So for doing that, we create a class named weights. There are 3 parameters, input_size which defines the number of perceptrons in the input layer. input_size = number of states(nS). The output size is the number of perceptrons in the output_layer. output_size = 1(which gives us the state distribution ratio for a particular state).\n",
    "\n",
    "Later when we want to update or find the state distribution ratio of any state, just pass that state to the forward(). First that particular state is converted into one_hot vector and then fed to the network. Finally the network returns the output value as the ratio of state distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "107177cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weights(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size = 0):\n",
    "        super(weights,self).__init__()\n",
    "        self.input_size = input_size;\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.output_size = output_size;\n",
    "        if(hidden_size!=0):\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size, bias=False)\n",
    "            self.linear2 = nn.Linear(self.hidden_size, self.output_size, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.input_size, self.output_size, bias=False)\n",
    "    '''\n",
    "        forward(): We accept a state 's' as input. Then we convert this into one hot encoding which is accomplished by first two lines.\n",
    "        Further we convert this one_hot vector 's' into pytorch tensor and then pass it through the network to obtain a output which is returned \n",
    "    '''\n",
    "    def forward(self,state):\n",
    "        s = np.zeros(self.input_size);\n",
    "        #print(state,end='===>');\n",
    "        s[state] = 1;\n",
    "        state = torch.FloatTensor(s).to(device)\n",
    "        #print(state);\n",
    "        if(self.hidden_size == 0):\n",
    "            output = torch.exp(self.linear1(state)) #To ensure that the outputs are always positive. giving Relu will cause problems.\n",
    "        else:\n",
    "            output = torch.exp(self.linear2(torch.exp(self.linear1(state))));\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6455065f",
   "metadata": {},
   "source": [
    "Now it is evident that the network written above will give me the ratio of state distribution. But the requirement is state distribution of the target_policy. So for doing that we need to to follow the below equation\n",
    "\n",
    "state_distribution_of_target_policy = Normalize(state_distribution_ratio_obtained_from_network * behaviour_policy_state_distribution).\n",
    "\n",
    "Now to get the target_policy_state_distribution, we need to obtain the behaviour_policy_state_distribution. To find that we use the class below. Now using the"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62c378e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class beh_pol_sd:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,onehot_encode=1):\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d41d6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target_policy,nS,nA):\n",
    "    one_hot_tp = [];\n",
    "    for i in range(len(target_policy)):\n",
    "        policy = target_policy[i];\n",
    "        print(policy);\n",
    "        tp=np.zeros((nS,nA));\n",
    "        for j in range(nS):\n",
    "            tp[j][policy[j]] = 1;\n",
    "        one_hot_tp.append(tp);\n",
    "    return np.array(one_hot_tp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7399475f",
   "metadata": {},
   "source": [
    "Now that we are all ready got our ingedients, let us define a separate class where we will define the average_case technique to find the state_distribution_ratio. Any object of the average_case class must possess the environment details such as number of states(nS) and number of actions(nA). The behaviour_policy, learning_rate, a weight_object to refer to the weights class. The batch_size and the optimizer to be used(Adam_optimzer).\n",
    "\n",
    "Since in this variant, we will be having only nS Neural Network and update the network corresponding to the sample that is obtained from the theta values. Hence, we define our first function\n",
    "\n",
    "1) set_target_policy: Since each neural network corresponds to a particular policy or each neural network corresponds to a particular target_policy, so we need to set the target_policy first. This function is mainly responsible for setting the target policy corresponding to which this neural network is created for. Also we declare the optimizer(Here Adam optimizer), batch_size etc.\n",
    "\n",
    "2) show_policy(): This function simply return the target_policy that the network is catering to.\n",
    "\n",
    "3) set_batch(): This function is used to set a data batch which is sampled from the behaviour_policy. Now the batch is set to be used for updating the state_distribution_ratio.\n",
    "\n",
    "4) get_batch(): This function is used to get a random batch of 50 samples from the set data to be used to update our state_distribution_ratio. We actually create a batch of 50 data samples 1000 times in order to reach to a good value of state_distribution_ratio(Like in Linear regression we use a batch for several times until our gradient converges).\n",
    "\n",
    "5) get_w(): This function is used to find the numerator and denominator of the loss function as mentioned in the paper 'Breaking the curse of horizon'. Now for finding the numerator paarameter pair = 0. To find the denominator pair = 1. Now it is observed that the denominator value easily goes to 0. So, to avoid divide by zero error, we add a small noise value of 0.000000001. This makes sure that the denominator value never goes to zero.\n",
    "\n",
    "6) get_state_distribution(): This function uses the set data in the self.set_data() to get batches of size 50. Then calculate the loss using the equation mentioned in the paper 'Breaking the curse of horizon'. We use this calculated loss to update our weights of the Neural network by using Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "854e0ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_case_distribution:\n",
    "    def __init__(self,nS,nA,behaviour_policy,state,lr):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.behaviour_policy = behaviour_policy;\n",
    "        self.state = state;\n",
    "        self.lr = lr\n",
    "        self.W_loss = 0\n",
    "        self.weight_obj = weights(nS,1).to(device);\n",
    "    def set_target_policy(self,target_pol):\n",
    "        self.target_policy = target_pol;\n",
    "        self.optimizerW = optim.Adam(self.weight_obj.parameters(),lr = self.lr);\n",
    "        self.batch_size = 50\n",
    "        self.batch=[];\n",
    "    def show_policy(self):\n",
    "        print(self.target_policy);\n",
    "    def set_batch(self,data):\n",
    "        self.data = data;\n",
    "        self.T = len(data);\n",
    "    def get_batch(self):\n",
    "        if(self.T<=50):\n",
    "            return self.data\n",
    "        else:\n",
    "            i = 1;\n",
    "            j = np.random.choice(self.T);\n",
    "            batch = [];\n",
    "            while(i<=self.batch_size):\n",
    "                if(np.random.random()<=0.5):\n",
    "                    batch.append([self.data[j][0],self.data[j][1],self.data[j][2]])\n",
    "                    j = (j+1)%self.T;\n",
    "                    i+=1;\n",
    "            return batch;\n",
    "    \n",
    "    def get_w(self,data,weight_obj,m,pair=0):\n",
    "        if(pair == 1):\n",
    "            Z_w_state = 0;\n",
    "            for i in range(len(data)):\n",
    "                val = weight_obj(data[i][0]);\n",
    "                #print(val);\n",
    "                Z_w_state+=val;\n",
    "            #print(Z_w_state.detach().numpy()[0]/self.batch_size);\n",
    "            return Z_w_state.cpu().detach().numpy()[0]/self.batch_size+0.0000000001;\n",
    "        else:\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2 = list(),list(),list(),list(),list(),list(),list(),list();\n",
    "            K = list();\n",
    "            for i in range(len(data)):\n",
    "                sample1 = data[i][0];\n",
    "                sample2 = data[i][1];\n",
    "                state1.append(sample1[0]);\n",
    "                #print(sample1);\n",
    "                w_state1.append(weight_obj(sample1[0]));\n",
    "                w_next_state1.append(weight_obj(sample1[2]));\n",
    "                state2.append(sample2[0]);\n",
    "                w_state2.append(weight_obj(sample2[0]));\n",
    "                w_next_state2.append(weight_obj(sample2[2]));\n",
    "                beta1.append(self.target_policy[sample1[0],sample1[1]]/self.behaviour_policy[sample1[0],sample1[1]]);\n",
    "                beta2.append(self.target_policy[sample2[0],sample2[1]]/self.behaviour_policy[sample2[0],sample2[1]]);\n",
    "                K.append(sample1[2]==sample2[2]);\n",
    "            return (state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K);\n",
    "    \n",
    "    def get_state_distribution_ratio(self,run,t):\n",
    "        #self.batch = self.get_batch();\n",
    "        eps = 0.04;\n",
    "        for _ in range(1000):\n",
    "            self.batch = self.get_batch();\n",
    "            pairs = list(product(self.batch,repeat=2));\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K = self.get_w(pairs, self.weight_obj, len(self.batch));\n",
    "            Z_w_state = self.get_w(self.batch, self.weight_obj, len(self.batch),1);\n",
    "            self.w_loss = 0\n",
    "            for i in range(len(state1)):\n",
    "                self.w_loss+=(beta1[i]*(w_state1[i]/Z_w_state) - (w_next_state1[i]/Z_w_state))*(beta2[i]*(w_state2[i]/Z_w_state)-(w_next_state2[i]/Z_w_state))*K[i];\n",
    "            self.w_loss/=(2*self.batch_size);\n",
    "            if(self.w_loss<eps):\n",
    "                break;\n",
    "            self.optimizerW.zero_grad();\n",
    "            self.w_loss.backward();\n",
    "            self.optimizerW.step();\n",
    "            self.optimizerW.zero_grad();\n",
    "        state_dist=[];\n",
    "        for i in range(self.nS):\n",
    "            w_state = self.weight_obj(i);\n",
    "            w_state = w_state.cpu().detach().numpy()[0];\n",
    "            state_dist.append(w_state);\n",
    "        return np.array(state_dist);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c265f1",
   "metadata": {},
   "source": [
    "Instead of sampling state, action, next_state values on the go, we do it before hand and store it in a list named 'data'. So, that when required we can simply pass this data and save some time by before hand sampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72edb88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(T,state):\n",
    "  global P,behaviour_policy,batch_size;\n",
    "  data=[];temp=[];\n",
    "  for t in range(T):\n",
    "    if(t%batch_size==0):\n",
    "      data.append(temp);\n",
    "    action = np.argmax(np.random.multinomial(1,behaviour_policy[state,:]))\n",
    "    next_state = np.argmax(np.random.multinomial(1,P[action,state,:]));\n",
    "    state = next_state;\n",
    "    temp.append([state,action,next_state]);\n",
    "  return data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e19992",
   "metadata": {},
   "source": [
    "For our help we create a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf350f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(theta):\n",
    "  theta = np.exp(theta);\n",
    "  sum = np.sum(theta)\n",
    "  return theta/sum;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd93dbc",
   "metadata": {},
   "source": [
    "Finally call the hyperparameters, create the environment and start execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96ed842f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1]\n",
      " [0 0 1 1]\n",
      " [0 1 1 1]\n",
      " [1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "T,runs,lr,batch_size,state,nS,nA,rep_cost,alpha,gamma = get_hyperparameters().ret_hyperparameters();\n",
    "nPOL = nS;\n",
    "T_update = int(T/batch_size);\n",
    "mr_obj = Machine_Replacement(rep_cost,nS,nA);\n",
    "P,R = mr_obj.gen_probability(),mr_obj.gen_expected_reward()\n",
    "theta = np.ones(nPOL);\n",
    "behaviour_policy = np.ones((nS,nA))*0.5;\n",
    "behaviour_policy_state_distribution = beh_pol_sd(P,behaviour_policy,nS,nA).state_distribution_simulated(0);\n",
    "data = simulate_episode(T,state);\n",
    "with open('data_used','wb') as f:\n",
    "  pickle.dump(data,f);\n",
    "target_policy = np.ones((nPOL,nS),dtype = np.int8)\n",
    "for i in range(nPOL-1,0,-1):\n",
    "    target_policy[nPOL-i-1][0:i] = 0;\n",
    "print(target_policy)\n",
    "val2 = []\n",
    "for t in target_policy:\n",
    "    val2.append(sum([R[t[s],s] for s in range(nS)]))\n",
    "val2 = np.array(val2);\n",
    "runs = 5\n",
    "#policy_selected = np.zeros((T_update,runs))\n",
    "policy_sampled = np.zeros((T_update,runs))\n",
    "theta_change = []\n",
    "estimated_value = np.zeros((T_update,runs))\n",
    "loss = np.zeros((T_update,runs))\n",
    "val = 100;\n",
    "lr = 1;\n",
    "data_dict={0:[],1:[],2:[],3:[],4:[]};"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69e0761",
   "metadata": {},
   "source": [
    "We next start our process to perform the theta update. After every val=50 times, our learning rate is divided by 10. So if we start with lr = 1. After 50 instances, lr = 0.1. After 100 steps lr = 0.01 and so on... Now at each instant, we found the softmax of the theta values. Now from the given probability values we sample a policy that will be updated next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4ae85723",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 1]\n",
      "[0 0 1 1]\n",
      "[0 1 1 1]\n",
      "[1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                  | 0/1000 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|                                       | 1/1000 [01:01<17:11:55, 61.98s/it]\u001b[A\n",
      "  0%|                                       | 2/1000 [02:00<16:38:58, 60.06s/it]\u001b[A\n",
      "  0%|                                       | 3/1000 [02:58<16:18:06, 58.86s/it]\u001b[A\n",
      "  0%|▏                                       | 4/1000 [02:59<9:57:53, 36.02s/it]\u001b[A\n",
      "  0%|▏                                      | 5/1000 [03:57<13:08:51, 47.57s/it]\u001b[A\n",
      "  0%|                                                     | 0/5 [03:57<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_918020/1441935454.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtheta_change\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mw_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_policy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_obj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msampled_policy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_distribution_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbehaviour_policy_state_distribution\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_918020/319536543.py\u001b[0m in \u001b[0;36mget_state_distribution_ratio\u001b[0;34m(self, run, t)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_state1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_state2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_next_state1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw_next_state2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m             \u001b[0mZ_w_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_918020/319536543.py\u001b[0m in \u001b[0;36mget_w\u001b[0;34m(self, data, weight_obj, m, pair)\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mstate1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0;31m#print(sample1);\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m                 \u001b[0mw_state1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m                 \u001b[0mw_next_state1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0mstate2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_918020/2857120708.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     15\u001b[0m     '''\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;31m#print(state,end='===>');\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in tqdm(range(runs)):\n",
    "  val = 100;\n",
    "  lr = 1;\n",
    "  theta = np.ones(nPOL);\n",
    "  w_obj = [average_case_distribution(nS,nA,behaviour_policy,state,lr) for _ in range(nPOL)];\n",
    "  one_hot_target_policy = one_hot(target_policy,nS,nA)\n",
    "  c=0;\n",
    "  for i in range(nPOL):\n",
    "    w_obj[i].set_target_policy(one_hot_target_policy[i]);\n",
    "  for t in tqdm(range(T_update)):\n",
    "    if(t%val==0):\n",
    "      lr = lr/10;\n",
    "      #val+=t;\n",
    "    P_i = softmax(theta);\n",
    "    #print(P_i)\n",
    "    #selected_policy = np.argmax(P_i);\n",
    "    sampled_policy = np.argmax(np.random.multinomial(1,P_i));\n",
    "    selected_policy = sampled_policy\n",
    "    #policy_selected[t,run]=selected_policy;\n",
    "    policy_sampled[t,run] = sampled_policy;\n",
    "    #loss[t,run] = w_obj.w_loss.cpu().detach().numpy()[0];\n",
    "    theta_change.append(theta)\n",
    "    w_obj[sampled_policy].set_batch(data[t]);\n",
    "    sd = w_obj[sampled_policy].get_state_distribution_ratio(run,t);\n",
    "    sd = sd * behaviour_policy_state_distribution;\n",
    "    sd = sd/np.sum(sd)\n",
    "    rho_i = sum([sd[s]*R[target_policy[sampled_policy,s],s] for s in range(nS)]);\n",
    "    data_dict[run] =data_dict[run]+w_obj[sampled_policy].batch;\n",
    "    for t in range(len(theta)):\n",
    "        r = sum([R[target_policy[t,s],s] for s in range(nS)])\n",
    "        c=0;\n",
    "        theta = theta + lr *(P_i[sampled_policy]*(val2-4*c)-r+c);\n",
    "  print(\"One run completed\")\n",
    "\n",
    "#pd.DataFrame(policy_selected).to_excel(\"Policy_selection.xlsx\");\n",
    "pd.DataFrame(np.array(theta_change)).to_excel(\"Theta_values.xlsx\");\n",
    "pd.DataFrame(policy_sampled).to_excel(\"Policy_sampling.xlsx\");\n",
    "pd.DataFrame(data_dict).to_excel(\"Data_used.xlsx\");\n",
    "pd.DataFrame(estimated_value).to_excel(\"Estimated_Value_functions.xlsx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0093542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
