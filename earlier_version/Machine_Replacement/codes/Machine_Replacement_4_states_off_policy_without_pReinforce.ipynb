{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn',True)\n",
    "torch.multiprocessing.set_start_method('spawn',True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Machine_Replacement:\n",
    "    def __init__(self,rep_cost=0.7,nS=6,nA=2):\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "        self.cost = np.linspace(0.1, 0.99,nS);\n",
    "        self.rep_cost = rep_cost;\n",
    "    def gen_probability(self):\n",
    "        self.P = np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            for j in range(self.nS):\n",
    "                if(i<=j):\n",
    "                    self.P[0,i,j]=(i+1)*(j+1);\n",
    "                else:\n",
    "                    continue;\n",
    "            self.P[0,i,:]=self.P[0,i,:]/np.sum(self.P[0,i,:])\n",
    "            self.P[1,i,0]=1;\n",
    "        return self.P;\n",
    "    def gen_reward(self):\n",
    "        self.R=np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i,:] = self.cost[i];\n",
    "            self.R[1,i,0] = self.rep_cost+self.cost[0];\n",
    "        return self.R;\n",
    "    def gen_expected_reward(self):\n",
    "        self.R = np.zeros((self.nA,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i] = self.cost[i];\n",
    "            self.R[1,i] = self.rep_cost + self.cost[0];\n",
    "        return self.R;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Target_Policy:\n",
    "    '''\n",
    "        First we create an initiualizer function namely a constructor to initialize the variables\n",
    "        with initial data values\n",
    "    '''\n",
    "    def __init__(self,S,A,P,R,start):\n",
    "        self.S=S # represant the states of the MDP\n",
    "        self.nS = len(S) # Reperesants the number of states of the MDP\n",
    "        self.nA = len(A);# Represants the number of actions in the MDP\n",
    "        self.P=P # Represants the true Probability function\n",
    "        self.R=R # Represants the true Reward  function\n",
    "        self.A=A;# Represnats the Action Space\n",
    "        self.K_pol = []; # To store all the policies\n",
    "        self.s_start=start # Store the start state \n",
    "    '''\n",
    "        In the generate_next_state(), we are generating our next state to be visited based on the following input parameters\n",
    "        s : Current state\n",
    "        a : Current action\n",
    "    '''    \n",
    "    def generate_next_state(self,s,a):\n",
    "        #p = np.zeros(self.nS)\n",
    "        p = self.P[a][s][:] # extrcat all the probabilities of the different statestransition given current s and a\n",
    "        #print(p);\n",
    "        return (np.argmax(np.random.multinomial(1,p)))\n",
    "    \n",
    "    '''\n",
    "        Single function to find the plot between the cumulative regret generated by different algorithms\n",
    "        Parameters:\n",
    "            reg_list : A list containing the regret value at different runs instances averaged over several time\n",
    "    '''    \n",
    "    def plot_data(self,reg_list):\n",
    "        plt.plot(np.arange(len(reg_list)),np.cumsum(np.array(reg_list)),marker = '+'); \n",
    "    '''\n",
    "        Function to find the optimum policy out of the K policies found out.\n",
    "        Parameters:\n",
    "            runs : To find for how many runs the current policy to be runned\n",
    "            T : Each run consisiting of how many time steps to find the average reward for each policy in one run\n",
    "            Time complexity : O(#(policies) x #(episode runs) x #(number of Time steps in one episode))\n",
    "    '''\n",
    "    def find_optimum_policy(self):\n",
    "        self.find_policies(); #Call the find_policies() to find all the policies and store it in 'self.K' list\n",
    "        final_R = np.zeros(len(self.K_pol));\n",
    "        for idx,pol in enumerate(self.K_pol):\n",
    "            #policy = self.one_hot(pol);\n",
    "            beh_obj = beh_pol_sd(self.P, pol, self.nS, self.nA)\n",
    "            state_distribution = beh_obj.state_distribution_simulated(1);\n",
    "            final_R[idx] = sum([state_distribution[state] *self.R[int(pol[state]),state] for state in range(self.nS)]);\n",
    "        for l_pol in range(len(self.K_pol)):\n",
    "            print(self.K_pol[l_pol],\"    ====>    \",final_R[l_pol]); # Display the the expected reward for each policy\n",
    "        return (final_R,self.K_pol[np.argmin(final_R)],np.min(final_R));# Return the minimum reward, the policy number which gives the minimum reward and the policy that gives minimum reward\n",
    "    \n",
    "    def find_policies(self):\n",
    "        self.K_pol = [];\n",
    "        pol=np.zeros(self.nS) # First policy is all 0's\n",
    "        self.K_pol.append(np.array(pol)); # append it to our K_policy list namely self.K\n",
    "        for i in reversed(range(self.nS)):\n",
    "            pol[i] = 1; # Come from the end and since the structure is thresholding nature so make each position 1 from 0 and append the same\n",
    "            print(pol);\n",
    "            self.K_pol.append(np.array(pol));\n",
    "        print(len(self.K_pol),\" policies found\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.T = 500000;\n",
    "        self.runs = 10;\n",
    "        self.lr = 0.1;\n",
    "        self.batch_size = 50;\n",
    "        self.start = 0;\n",
    "        self.nS = 4;\n",
    "        self.nA = 2;\n",
    "        self.rep_cost = 0.7\n",
    "        self.alpha = 0.2\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def ret_hyperparameters(self):\n",
    "        return (self.T,self.runs,self.lr,self.batch_size,self.start,self.nS,self.nA,self.rep_cost,self.alpha,self.gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class beh_pol_sd:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,onehot_encode=1):\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "'''\n",
    "    The class weights is created to define the neural network structure.\n",
    "    Inputs:\n",
    "    -------\n",
    "    input_size  : number of input head perceptrons. Basically is equal to number of states nS.\n",
    "    output_size : number of perceptrons in the output layer.\n",
    "'''\n",
    "class weights(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size = 0):\n",
    "        super(weights,self).__init__()\n",
    "        self.input_size = input_size;\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.output_size = output_size;\n",
    "        if(hidden_size!=0):\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size, bias=False)\n",
    "            self.linear2 = nn.Linear(self.hidden_size, self.output_size, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.input_size, self.output_size, bias=False)\n",
    "    '''\n",
    "        forward(): We accept a state 's' as input. Then we convert this into one hot encoding which is accomplished by first two lines.\n",
    "        Further we convert this one_hot vector 's' into pytorch tensor and then pass it through the network to obtain a output which is returned \n",
    "    '''\n",
    "    def forward(self,state):\n",
    "        s = np.zeros(self.input_size);\n",
    "        #print(state,end='===>');\n",
    "        s[state] = 1;\n",
    "        state = torch.FloatTensor(s).to(device)\n",
    "        #print(state);\n",
    "        if(self.hidden_size == 0):\n",
    "            output = torch.exp(self.linear1(state)) #To ensure that the outputs are always positive. giving Relu will cause problems.\n",
    "        else:\n",
    "            output = torch.exp(self.linear2(torch.exp(self.linear1(state))));\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_case_distribution:\n",
    "    def __init__(self,nS,nA,behaviour_policy,state,lr):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.behaviour_policy = behaviour_policy;\n",
    "        self.state = state;\n",
    "        self.lr = lr\n",
    "        self.W_loss = 0\n",
    "        self.weight_obj = weights(nS,1).to(device);\n",
    "        self.W_loss = 0;\n",
    "    def set_target_policy(self,target_pol):\n",
    "        self.target_policy = target_pol;\n",
    "        self.optimizerW = optim.Adam(self.weight_obj.parameters(),lr = self.lr);\n",
    "        self.batch_size = 50\n",
    "    def show_policy(self):\n",
    "        print(self.target_policy);\n",
    "    def set_batch(self,data):\n",
    "        self.data = data;\n",
    "        self.T = len(data);\n",
    "    def get_batch(self):\n",
    "        if(self.T<=50):\n",
    "            return self.data\n",
    "        else:\n",
    "            i = 1;\n",
    "            j = np.random.choice(self.T);\n",
    "            batch = [];\n",
    "            while(i<=self.batch_size):\n",
    "                if(np.random.random()<=0.5):\n",
    "                    batch.append([self.data[j][0],self.data[j][1],self.data[j][2]])\n",
    "                    j = (j+1)%self.T;\n",
    "                    i+=1;\n",
    "            return batch;\n",
    "    \n",
    "    def get_w(self,data,weight_obj,m,pair=0):\n",
    "        if(pair == 1):\n",
    "            Z_w_state = 0;\n",
    "            for i in range(len(data)):\n",
    "                val = weight_obj(data[i][0]);\n",
    "                #print(val);\n",
    "                Z_w_state+=val;\n",
    "            #print(Z_w_state.detach().numpy()[0]/self.batch_size);\n",
    "            return Z_w_state.cpu().detach().numpy()[0]/self.batch_size;\n",
    "        else:\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2 = list(),list(),list(),list(),list(),list(),list(),list();\n",
    "            K = list();\n",
    "            for i in range(len(data)):\n",
    "                sample1 = data[i][0];\n",
    "                sample2 = data[i][1];\n",
    "                state1.append(sample1[0]);\n",
    "                #print(sample1);\n",
    "                w_state1.append(weight_obj(sample1[0]));\n",
    "                w_next_state1.append(weight_obj(sample1[2]));\n",
    "                state2.append(sample2[0]);\n",
    "                w_state2.append(weight_obj(sample2[0]));\n",
    "                w_next_state2.append(weight_obj(sample2[2]));\n",
    "                beta1.append(self.target_policy[sample1[0],sample1[1]]/self.behaviour_policy[sample1[0],sample1[1]]);\n",
    "                beta2.append(self.target_policy[sample2[0],sample2[1]]/self.behaviour_policy[sample2[0],sample2[1]]);\n",
    "                K.append(sample1[2]==sample2[2]);\n",
    "            return (state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K);\n",
    "    \n",
    "    def get_state_distribution_ratio(self):\n",
    "        batch = self.get_batch();\n",
    "        pairs = list(product(batch,repeat=2));\n",
    "        state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K = self.get_w(pairs, self.weight_obj, len(batch));\n",
    "        Z_w_state = self.get_w(batch, self.weight_obj, len(batch),1);\n",
    "        self.w_loss = 0\n",
    "        for i in range(len(state1)):\n",
    "            self.w_loss+=(beta1[i]*(w_state1[i]/Z_w_state) - (w_next_state1[i]/Z_w_state))*(beta2[i]*(w_state2[i]/Z_w_state)-(w_next_state2[i]/Z_w_state))*K[i];\n",
    "        self.w_loss/=(2*self.batch_size);\n",
    "        self.optimizerW.zero_grad();\n",
    "        self.w_loss.backward();\n",
    "        self.optimizerW.step();\n",
    "        self.optimizerW.zero_grad();\n",
    "        state_dist=[];\n",
    "        for i in range(self.nS):\n",
    "            w_state = self.weight_obj(i);\n",
    "            w_state = w_state.cpu().detach().numpy()[0];\n",
    "            state_dist.append(w_state);\n",
    "        return np.array(state_dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target_policy,nS,nA):\n",
    "    one_hot_tp = [];\n",
    "    for i in range(len(target_policy)):\n",
    "        policy = target_policy[i];\n",
    "        print(policy);\n",
    "        tp=np.zeros((nS,nA));\n",
    "        for j in range(nS):\n",
    "            tp[j][policy[j]] = 1;\n",
    "        one_hot_tp.append(tp);\n",
    "    return np.array(one_hot_tp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(T,run,T_update,batch_size,nS,nA,behaviour_policy,target_policy,one_hot_target_policy):\n",
    "    nPOL = nS\n",
    "    lr =0.1\n",
    "    ac_obj = list();\n",
    "    value_function=np.zeros(T_update)\n",
    "    #start = 0;\n",
    "    for i,pol in enumerate(one_hot_target_policy):\n",
    "        ac_obj.append( average_case_distribution(nS, nA, behaviour_policy, 0, lr))\n",
    "        ac_obj[i].set_target_policy(pol);\n",
    "    data = list();\n",
    "    policy_chose=np.zeros(nS)\n",
    "    state = 0;\n",
    "    #state_distribution = np.array([[np.random.random() for _ in range(nS)] for i in range(nPOL)])\n",
    "    print(\"Running\")\n",
    "    for t in range(1,T+batch_size+1):\n",
    "        if(t%batch_size==0):\n",
    "            #c+=1;\n",
    "            #print(k);\n",
    "            min_pol=np.zeros(nPOL);\n",
    "            for i in range(nPOL):\n",
    "                ac_obj[i].set_batch(np.array(data));\n",
    "                sd = ac_obj[i].get_state_distribution_ratio();\n",
    "                sd = sd*behaviour_state_distribution;\n",
    "                sd = sd/np.sum(sd);\n",
    "                #sd = state_distribution[i];\n",
    "                #print(i,\"==>\",sd);\n",
    "                min_pol[i]=sum([R[target_policy[i,s],s]*sd[s] for s in range(nS)]);\n",
    "            k = np.argmin(min_pol);\n",
    "            #print(min_pol,k);\n",
    "            policy_chose[k]+=1;\n",
    "            vf = true_value_function_list[k+1];\n",
    "            #data_dict[run]=data_dict[run]+data;\n",
    "            #data = [];\n",
    "            value_function[int(t/batch_size)-1] = vf;\n",
    "            #print(k,\" ==> \",vf);\n",
    "            #input();\n",
    "        action = np.argmax(np.random.multinomial(1,behaviour_policy[state,:]));\n",
    "        next_state = np.argmax(np.random.multinomial(1,P[action,state,:]));\n",
    "        data.append([state,action,next_state]);\n",
    "        state = next_state;\n",
    "    with open(\"Machine_rep_\"+str(nS)+\"_states_data_run\"+str(run),'wb') as f:\n",
    "        pickle.dump(data,f);\n",
    "    f.close();\n",
    "    with open(\"Machine_rep_\"+str(nS)+\"_states_value_func\"+str(run),'wb') as f:\n",
    "        pickle.dump(value_function,f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500000, 10, 0.1, 50, 0, 4, 2, 0.7, 0.2, 0.95)\n",
      "[0. 0. 0. 1.]\n",
      "[0. 0. 1. 1.]\n",
      "[0. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "5  policies found\n",
      "[0. 0. 0. 0.]     ====>     0.9900000000000003\n",
      "[0. 0. 0. 1.]     ====>     0.4907944514501892\n",
      "[0. 0. 1. 1.]     ====>     0.42741721854304665\n",
      "[0. 1. 1. 1.]     ====>     0.4315789473684213\n",
      "[1. 1. 1. 1.]     ====>     0.7999999999999999\n",
      "[0 0 0 1]\n",
      "[0 0 1 1]\n",
      "[0 1 1 1]\n",
      "[1 1 1 1]\n",
      "Running\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-0eed652e7625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mT_update\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mT_update\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbehaviour_policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget_policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mone_hot_target_policy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-30-c005f08f6e09>\u001b[0m in \u001b[0;36mprocessing\u001b[1;34m(T, run, T_update, batch_size, nS, nA, behaviour_policy, target_policy, one_hot_target_policy)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnPOL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m                 \u001b[0mac_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                 \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mac_obj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_state_distribution_ratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                 \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbehaviour_state_distribution\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m                 \u001b[0msd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msd\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-d5b66d47c8f9>\u001b[0m in \u001b[0;36mget_state_distribution_ratio\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_loss\u001b[0m\u001b[1;33m/=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizerW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mw_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizerW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizerW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    306\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 307\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__=='__main__':\n",
    "    T,runs,lr,batch_size,start,nS,nA,rep_cost,alpha,gamma = get_hyperparameters().ret_hyperparameters();\n",
    "    print(get_hyperparameters().ret_hyperparameters())\n",
    "    nPOL = nS;\n",
    "    eps = 0.08;\n",
    "    epsilon = 0.1;\n",
    "    mr_obj = Machine_Replacement(rep_cost,nS,nA);\n",
    "    P,R,R_comp = mr_obj.gen_probability(),mr_obj.gen_expected_reward(),mr_obj.gen_reward();\n",
    "    behaviour_policy = np.ones((nPOL,nA))*0.5\n",
    "    behaviour_state_distribution = beh_pol_sd(P, behaviour_policy, nS, nA).state_distribution_simulated(0)\n",
    "    target_policy = np.ones((nPOL,nS),dtype = np.int8)\n",
    "    for i in range(nPOL):\n",
    "        target_policy[i][:-(i+1)] = 0;\n",
    "\n",
    "    epsilon_optimal_cost_matrix = np.zeros((int(T/batch_size),runs));\n",
    "    value_function = np.zeros((int(T/batch_size),runs));\n",
    "    #q_learning_optimal_cost_matrix = np.zeros((T,runs));\n",
    "    tp=Target_Policy(np.arange(nS), np.arange(nA), P, R, start)\n",
    "    true_value_function_list,optimal_policy,optimal_value_function = tp.find_optimum_policy();\n",
    "    one_hot_target_policy = one_hot(target_policy,nS,nA);\n",
    "    policy_chose=np.zeros(nS)\n",
    "    ac_obj = list();\n",
    "    T_update = int(T/batch_size);\n",
    "    for run in range(runs):\n",
    "        processing(T,run,T_update,batch_size,nS,nA,behaviour_policy,target_policy,one_hot_target_policy);\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
