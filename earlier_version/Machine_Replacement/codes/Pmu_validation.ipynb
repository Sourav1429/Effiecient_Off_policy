{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2c3973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class River_Swim:\n",
    "    def __init__(self,nS=6,nA=2):\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    def gen_probability(self):\n",
    "        P = np.zeros((self.nA,self.nS,self.nS));\n",
    "        P[0,0,0],P[0,0,1] = 0.9,0.1\n",
    "        P[1,self.nS-1,self.nS-1],P[1,self.nS-1,self.nS-2] = 0.4,0.6\n",
    "        for i in range(1,self.nS-1):\n",
    "            P[0,i,i-1],P[0,i,i],P[0,i,i+1] = 0.3,0.6,0.1\n",
    "            P[1,self.nS-i-1,self.nS-i],P[1,self.nS-i-1,self.nS-i-1],P[1,self.nS-i-1,self.nS-i-2] = 0.3,0.6,0.1\n",
    "        P[0,self.nS-1,self.nS-1],P[0,self.nS-1,self.nS-2] = 0.7,0.3\n",
    "        P[1,0,0],P[1,0,1] = 0.7,0.3\n",
    "        #print(P)\n",
    "        #print(\"Reward function\");\n",
    "        return P;\n",
    "    def gen_expected_reward(self):\n",
    "        R = np.zeros((self.nA,self.nS));\n",
    "        R[0,0] = 1\n",
    "        R[1,self.nS-1] = 0.82\n",
    "        #print(R);\n",
    "        return R;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "545daba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class beh_pol_sd:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "        print(self.nS);\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,onehot_encode=1):\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fa3ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class find_D_mu:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        #print(self.policy)\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,P_policy,policy,onehot_encode=1):\n",
    "        self.policy = policy;\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "027bb2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nS = 6\n",
    "nA = 2\n",
    "batch_size = 50;\n",
    "estimated_P_freq = np.zeros((nA,nS,nS));\n",
    "behaviour_policy = np.ones((nS,nA))*0.5\n",
    "rep_cost = 0.7\n",
    "mr_obj = River_Swim(nS,nA)\n",
    "P,R = mr_obj.gen_probability(),mr_obj.gen_expected_reward()\n",
    "state = 0;\n",
    "runs = 1;\n",
    "T = 100\n",
    "\n",
    "state = 0;\n",
    "final_list=[];\n",
    "temporary_list=[];\n",
    "P_est = np.zeros((nA,nS,nS));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bca7e836",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9, 0.1, 0. , 0. , 0. , 0. ],\n",
       "        [0.3, 0.6, 0.1, 0. , 0. , 0. ],\n",
       "        [0. , 0.3, 0.6, 0.1, 0. , 0. ],\n",
       "        [0. , 0. , 0.3, 0.6, 0.1, 0. ],\n",
       "        [0. , 0. , 0. , 0.3, 0.6, 0.1],\n",
       "        [0. , 0. , 0. , 0. , 0.3, 0.7]],\n",
       "\n",
       "       [[0.7, 0.3, 0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.6, 0.3, 0. , 0. , 0. ],\n",
       "        [0. , 0.1, 0.6, 0.3, 0. , 0. ],\n",
       "        [0. , 0. , 0.1, 0.6, 0.3, 0. ],\n",
       "        [0. , 0. , 0. , 0.1, 0.6, 0.3],\n",
       "        [0. , 0. , 0. , 0. , 0.6, 0.4]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5d2f65a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.8        0.2        0.         0.         0.         0.        ]\n",
      "  [0.33333333 0.66666667 0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         0.         1.         0.        ]\n",
      "  [0.         0.         0.         0.14285714 0.71428571 0.14285714]\n",
      "  [0.         0.         0.         0.         0.375      0.625     ]]\n",
      "\n",
      " [[0.8        0.2        0.         0.         0.         0.        ]\n",
      "  [0.         0.         1.         0.         0.         0.        ]\n",
      "  [0.         0.         0.         1.         0.         0.        ]\n",
      "  [0.         0.         0.         0.5        0.5        0.        ]\n",
      "  [0.         0.         0.         0.         0.66666667 0.33333333]\n",
      "  [0.         0.         0.         0.         0.5        0.5       ]]]\n",
      "[[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      "  [0.25       0.75       0.         0.         0.         0.        ]\n",
      "  [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      "  [0.         0.         0.18181818 0.72727273 0.09090909 0.        ]\n",
      "  [0.         0.         0.         0.13333333 0.73333333 0.13333333]\n",
      "  [0.         0.         0.         0.         0.38461538 0.61538462]]\n",
      "\n",
      " [[0.83333333 0.16666667 0.         0.         0.         0.        ]\n",
      "  [0.5        0.         0.5        0.         0.         0.        ]\n",
      "  [0.         0.         0.5        0.5        0.         0.        ]\n",
      "  [0.         0.         0.1        0.7        0.2        0.        ]\n",
      "  [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      "  [0.         0.         0.         0.         0.42857143 0.57142857]]]\n"
     ]
    }
   ],
   "source": [
    "for run in range(runs):\n",
    "    temporary_list=[];\n",
    "    estimated_P_freq = np.zeros((nA,nS,nS));\n",
    "    for t in range(1,T+1):\n",
    "        if(t%batch_size==0):\n",
    "            P_est = np.zeros((nA,nS,nS));\n",
    "            for s in range(nS):\n",
    "                if(np.sum(estimated_P_freq[0,s,:])>0):\n",
    "                    P_est[0,s,:]=estimated_P_freq[0,s,:]/np.sum(estimated_P_freq[0,s,:]);\n",
    "                if(np.sum(estimated_P_freq[1,s,:])>0):\n",
    "                    P_est[1,s,:]=estimated_P_freq[1,s,:]/np.sum(estimated_P_freq[1,s,:]);\n",
    "            print(P_est);\n",
    "            #input();\n",
    "            temporary_list.append(P_est);\n",
    "\n",
    "        action = np.argmax(np.random.multinomial(1,behaviour_policy[state,:]));\n",
    "        next_state = np.argmax(np.random.multinomial(1,P[action,state,:]));\n",
    "        estimated_P_freq[action,state,next_state]+=1;\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b32301a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1]\n",
      " [0 0 0 0 1 1]\n",
      " [0 0 0 1 1 1]\n",
      " [0 0 1 1 1 1]\n",
      " [0 1 1 1 1 1]\n",
      " [1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "nPOL=nS+1;\n",
    "target_policy = np.ones((nPOL,nS),dtype = np.int8)\n",
    "for i in range(nPOL-1,0,-1):\n",
    "    target_policy[nPOL-i-1][0:i] = 0;\n",
    "print(target_policy)\n",
    "obj = [find_D_mu(P, policy, nS, nA) for policy in target_policy];\n",
    "policy_wise_value_func = np.zeros((nPOL,T,runs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0659a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.25       0.75       0.         0.         0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.18181818 0.72727273 0.09090909 0.        ]\n",
      " [0.         0.         0.         0.13333333 0.73333333 0.13333333]\n",
      " [0.         0.         0.         0.         0.38461538 0.61538462]]\n",
      "[0.66758242 0.22252747 0.07417582 0.02472527 0.00824176 0.00274725]\n",
      "0.6675824175824215\n",
      "policy was:====> [0 0 0 0 0 0]\n",
      "====================================================\n",
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.25       0.75       0.         0.         0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.18181818 0.72727273 0.09090909 0.        ]\n",
      " [0.         0.         0.         0.13333333 0.73333333 0.13333333]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.66850069 0.22283356 0.07427785 0.02475928 0.00825309 0.00137552]\n",
      "0.6696286107290246\n",
      "policy was:====> [0 0 0 0 0 1]\n",
      "====================================================\n",
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.25       0.75       0.         0.         0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.18181818 0.72727273 0.09090909 0.        ]\n",
      " [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.65060241 0.21686747 0.07228916 0.02409639 0.02409639 0.01204819]\n",
      "0.6604819277108472\n",
      "policy was:====> [0 0 0 0 1 1]\n",
      "====================================================\n",
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.25       0.75       0.         0.         0.         0.        ]\n",
      " [0.         0.33333333 0.33333333 0.33333333 0.         0.        ]\n",
      " [0.         0.         0.1        0.7        0.2        0.        ]\n",
      " [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.48648649 0.16216216 0.05405405 0.05405405 0.16216216 0.08108108]\n",
      "0.5529729729729776\n",
      "policy was:====> [0 0 0 1 1 1]\n",
      "====================================================\n",
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.25       0.75       0.         0.         0.         0.        ]\n",
      " [0.         0.         0.5        0.5        0.         0.        ]\n",
      " [0.         0.         0.1        0.7        0.2        0.        ]\n",
      " [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.13953488 0.04651163 0.04651163 0.13953488 0.41860465 0.20930233]\n",
      "0.31116279069767494\n",
      "policy was:====> [0 0 1 1 1 1]\n",
      "====================================================\n",
      "[[0.85714286 0.14285714 0.         0.         0.         0.        ]\n",
      " [0.5        0.         0.5        0.         0.         0.        ]\n",
      " [0.         0.         0.5        0.5        0.         0.        ]\n",
      " [0.         0.         0.1        0.7        0.2        0.        ]\n",
      " [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.01834862 0.01834862 0.05504587 0.16513761 0.49541284 0.24770642]\n",
      "0.22146788990825647\n",
      "policy was:====> [0 1 1 1 1 1]\n",
      "====================================================\n",
      "[[0.83333333 0.16666667 0.         0.         0.         0.        ]\n",
      " [0.5        0.         0.5        0.         0.         0.        ]\n",
      " [0.         0.         0.5        0.5        0.         0.        ]\n",
      " [0.         0.         0.1        0.7        0.2        0.        ]\n",
      " [0.         0.         0.         0.05882353 0.58823529 0.35294118]\n",
      " [0.         0.         0.         0.         0.42857143 0.57142857]]\n",
      "[0.00619195 0.01857585 0.05572755 0.16718266 0.50154799 0.25077399]\n",
      "0.20563467492260043\n",
      "policy was:====> [1 1 1 1 1 1]\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "for index,pol in enumerate(target_policy):\n",
    "    P = np.array([P_est[pol[s],s,:] for s in range(nS)]);\n",
    "    print(P);\n",
    "    #print(i[0],i[1]);\n",
    "    sd = obj[index].state_distribution_simulated(P,pol)\n",
    "    vf = sum([R[pol[state],state]*sd[state] for state in range(nS)])\n",
    "    print(sd)\n",
    "    print(vf)\n",
    "    print(\"policy was:====>\",pol);\n",
    "    print(\"====================================================\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca265168",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
