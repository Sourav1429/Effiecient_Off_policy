{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34015286",
   "metadata": {},
   "source": [
    "We begin by importing all the necssary packages and modules. Next our task is to build the Machine_Replacement environment. For doing that, we create a class named as Machine_Replacement which accepts the number_of_states(nS), number_of_actions(nA) and replacement_cost(rep_cost) as input and generate the environment. Later we just need to cal the function gen_probability() and gen_expected_reward_function() to get the Probability distribution matrix and Reward matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "694033f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn',True)\n",
    "torch.multiprocessing.set_start_method('spawn',True)\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "class Machine_Replacement:\n",
    "    def __init__(self,rep_cost=0.7,nS=6,nA=2):\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "        self.cost = np.linspace(0.1, 0.99,nS);\n",
    "        self.rep_cost = rep_cost;\n",
    "    def gen_probability(self):\n",
    "        self.P = np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            for j in range(self.nS):\n",
    "                if(i<=j):\n",
    "                    self.P[0,i,j]=(i+1)*(j+1);\n",
    "                else:\n",
    "                    continue;\n",
    "            self.P[0,i,:]=self.P[0,i,:]/np.sum(self.P[0,i,:])\n",
    "            self.P[1,i,0]=1;\n",
    "        return self.P;\n",
    "    def gen_reward(self):\n",
    "        self.R=np.zeros((self.nA,self.nS,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i,:] = self.cost[i];\n",
    "            self.R[1,i,0] = self.rep_cost+self.cost[0];\n",
    "        return self.R;\n",
    "    def gen_expected_reward(self):\n",
    "        self.R = np.zeros((self.nA,self.nS));\n",
    "        for i in range(self.nS):\n",
    "            self.R[0,i] = self.cost[i];\n",
    "            self.R[1,i] = self.rep_cost + self.cost[0];\n",
    "        return self.R;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1476c06e",
   "metadata": {},
   "source": [
    "Next we create another class specifying the hyperparameters that might be required in our algorithm. Later when required we can just call ret_hyperparameters() to get the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "102e7d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_hyperparameters:\n",
    "    def __init__(self):\n",
    "        self.T = 50;\n",
    "        self.runs = 5;\n",
    "        self.lr = 0.1;\n",
    "        self.batch_size = 25;\n",
    "        self.start = 0;\n",
    "        self.nS = 4;\n",
    "        self.nA = 2;\n",
    "        self.rep_cost = 0.7\n",
    "        self.alpha = 0.2\n",
    "        self.gamma = 0.95\n",
    "    \n",
    "    def ret_hyperparameters(self):\n",
    "        return (self.T,self.runs,self.lr,self.batch_size,self.start,self.nS,self.nA,self.rep_cost,self.alpha,self.gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb9e77",
   "metadata": {},
   "source": [
    "Let us now define the pytorch model. So for doing that, we create a class named weights. There are 3 parameters, input_size which defines the number of perceptrons in the input layer. input_size = number of states(nS). The output size is the number of perceptrons in the output_layer. output_size = 1(which gives us the state distribution ratio for a particular state).\n",
    "\n",
    "Later when we want to update or find the state distribution ratio of any state, just pass that state to the forward(). First that particular state is converted into one_hot vector and then fed to the network. Finally the network returns the output value as the ratio of state distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0efa73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class weights(nn.Module):\n",
    "    def __init__(self,input_size,output_size,hidden_size = 0):\n",
    "        super(weights,self).__init__()\n",
    "        self.input_size = input_size;\n",
    "        self.hidden_size = hidden_size;\n",
    "        self.output_size = output_size;\n",
    "        if(hidden_size!=0):\n",
    "            self.linear1 = nn.Linear(self.input_size, self.hidden_size, bias=False)\n",
    "            self.linear2 = nn.Linear(self.hidden_size, self.output_size, bias=False)\n",
    "        else:\n",
    "            self.linear1 = nn.Linear(self.input_size, self.output_size, bias=False)\n",
    "    '''\n",
    "        forward(): We accept a state 's' as input. Then we convert this into one hot encoding which is accomplished by first two lines.\n",
    "        Further we convert this one_hot vector 's' into pytorch tensor and then pass it through the network to obtain a output which is returned \n",
    "    '''\n",
    "    def forward(self,state):\n",
    "        s = np.zeros(self.input_size);\n",
    "        #print(state,end='===>');\n",
    "        s[state] = 1;\n",
    "        state = torch.FloatTensor(s).to(device)\n",
    "        #print(state);\n",
    "        if(self.hidden_size == 0):\n",
    "            output = torch.exp(self.linear1(state)) #To ensure that the outputs are always positive. giving Relu will cause problems.\n",
    "        else:\n",
    "            output = torch.exp(self.linear2(torch.exp(self.linear1(state))));\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52610676",
   "metadata": {},
   "source": [
    "Now it is evident that the network written above will give me the ratio of state distribution. But the requirement is state distribution of the target_policy. So for doing that we need to to follow the below equation\n",
    "\n",
    "state_distribution_of_target_policy = Normalize(state_distribution_ratio_obtained_from_network * behaviour_policy_state_distribution).\n",
    "\n",
    "Now to get the target_policy_state_distribution, we need to obtain the behaviour_policy_state_distribution. To find that we use the class below. Now using the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "40ea96db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class beh_pol_sd:\n",
    "    def __init__(self,P,policy,nS,nA):\n",
    "        self.P = P\n",
    "        self.policy = policy\n",
    "        self.nS = nS;\n",
    "        self.nA = nA;\n",
    "    \n",
    "    def onehot(self):\n",
    "        pol = np.zeros((self.nS,self.nA));\n",
    "        for i in range(self.nS):\n",
    "            pol[i][int(self.policy[i])]=1;\n",
    "        return pol;\n",
    "    def find_transition_matrix(self,onehot_encode=1):\n",
    "        if(onehot_encode==1):\n",
    "            self.policy = self.onehot()\n",
    "        T_s_s_next = np.zeros((self.nS,self.nS));\n",
    "        for s in range(self.nS):\n",
    "            for s_next in range(self.nS):\n",
    "                for a in range(self.nA):\n",
    "                    #print(s,s_next,a);\n",
    "                    #print(T[a,s,s_next]);\n",
    "                    T_s_s_next[s,s_next]+=self.P[a,s,s_next]*self.policy[s,a];\n",
    "        return T_s_s_next;\n",
    "    def state_distribution_simulated(self,onehot_encode=1):\n",
    "        P_policy = self.find_transition_matrix(onehot_encode)\n",
    "        #print(P_policy);\n",
    "        P_dash = np.append(P_policy - np.eye(self.nS),np.ones((self.nS,1)),axis=1);\n",
    "        #print(P_dash);\n",
    "        P_last = np.linalg.pinv(np.transpose(P_dash))[:,-1]\n",
    "        return P_last;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3285f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(target_policy,nS,nA):\n",
    "    one_hot_tp = [];\n",
    "    for i in range(len(target_policy)):\n",
    "        policy = target_policy[i];\n",
    "        print(policy);\n",
    "        tp=np.zeros((nS,nA));\n",
    "        for j in range(nS):\n",
    "            tp[j][policy[j]] = 1;\n",
    "        one_hot_tp.append(tp);\n",
    "    return np.array(one_hot_tp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dea90b",
   "metadata": {},
   "source": [
    "Now that we are all ready got our ingedients, let us define a separate class where we will define the average_case technique to find the state_distribution_ratio. Any object of the average_case class must possess the environment details such as number of states(nS) and number of actions(nA). The behaviour_policy, learning_rate,list_of_beta_values, a weight_object to refer to the weights class. The batch_size and the optimizer to be used(Adam_optimzer). \n",
    "\n",
    "Since in this variant, we will be having only 1 Neural Network and update the network for the sample that is obtained from the theta values. For this reason, we need to store all the beta values. Hence, we define our first function\n",
    "1) find_beta(): This function finds all the beta(importance sampling) values. So beta values = target_policy[s,a]/behaviour_policy[s,a]. Now there are nPOL policies so for each policies we need to find the beta values for the ith target_policy, beta value will be beta[i,s,a]=target_policy[i,s,a]/behaviour_policy[s,a]. Now it is certain that a(action) can be {0,1} only. So instead of creating a new loop, we manually define two lines once for a=0 when beta value becomes beta[i,s,0]=target_policy[i,s,0]/behaviour_policy[s,0] and beta[i,s,1] = target_policy[i,s,1]/behaviour_policy[s,1]. Finally return all the beta values.\n",
    "\n",
    "2) set_batch(): This function is used to set a data batch which is sampled from the behaviour_policy. Now the batch is set to be used for updating the state_distribution_ratio.\n",
    "\n",
    "3) get_batch(): This function is used to get a random batch of 50 samples from the set data to be used to update our state_distribution_ratio. We actually create a batch of 50 data samples 10000 times in order to reach to a good value of state_distribution_ratio(Like in Linear regression we use a batch for several times until our gradient converges)\n",
    "\n",
    "4) get_w(): This function is used to find the numerator and denominator of the loss function as mentioned in the paper 'Breaking the curse of horizon'. Now for finding the numerator paarameter pair = 0. To find the denominator pair = 1. Now it is observed that the denominator value easily goes to 0. So, to avoid divide by zero error, we add a small noise value of 0.000000001. This makes sure that the denominator value never goes to zero.\n",
    "\n",
    "5) get_state_distribution_ratio(): This function uses the set data in the self.set_data() to get batches of size 50. Then calculate the loss using the equation mentioned in the paper 'Breaking the curse of horizon'. We use this calculated loss to update our weights of the Neural network by using Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26a55bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class average_case_version_2:\n",
    "  def __init__(self,nS,nA,behaviour_policy,state,lr,target_policy,batch_size,data_used):\n",
    "    self.nS = nS;\n",
    "    self.nPOL = nS\n",
    "    self.nA = nA;\n",
    "    self.behaviour_policy = behaviour_policy;\n",
    "    self.lr = lr;\n",
    "    self.beta = self.find_beta(target_policy,behaviour_policy)\n",
    "    self.W_loss = 0\n",
    "    self.weight_obj = weights(nS,1).to(device);\n",
    "    self.batch_size = batch_size\n",
    "    self.optimizerW = optim.Adam(self.weight_obj.parameters(),lr = self.lr);\n",
    "    self.loss = [];\n",
    "    self.Z = [];\n",
    "    self.data_used = data_used\n",
    "  def find_beta(self,target_policy,behaviour_policy):\n",
    "    beta = np.zeros((self.nPOL,self.nS,self.nA))\n",
    "    for i in range(self.nPOL):\n",
    "      for s in range(self.nS):\n",
    "        beta[i][s][0] = target_policy[i,s,0]/behaviour_policy[s,0];\n",
    "        beta[i,s,1] = target_policy[i,s,1]/behaviour_policy[s,1];\n",
    "    return beta;\n",
    "  def set_batch(self,data):\n",
    "        self.data = data;\n",
    "        self.T = len(data);\n",
    "  def get_batch(self):\n",
    "      if(self.T<=50):\n",
    "          return self.data\n",
    "      else:\n",
    "          i = 1;\n",
    "          j = np.random.choice(self.T);\n",
    "          batch = [];\n",
    "          while(i<=self.batch_size):\n",
    "              if(np.random.random()<=0.5):\n",
    "                  batch.append([self.data[j][0],self.data[j][1],self.data[j][2]])\n",
    "                  j = (j+1)%self.T;\n",
    "                  i+=1;\n",
    "          return batch; \n",
    "  def get_w(self,data,weight_obj,m,pair=0):\n",
    "        if(pair == 1):\n",
    "            Z_w_state = 0;\n",
    "            for i in range(len(data)):\n",
    "                val = weight_obj(data[i][0]);\n",
    "                #print(val);\n",
    "                Z_w_state+=val;\n",
    "            #print(Z_w_state.detach().numpy()[0]/self.batch_size);\n",
    "            Z_w_state = Z_w_state.cpu().detach().numpy()[0]/self.batch_size;\n",
    "            if(Z_w_state<0.00000000000005):\n",
    "                Z_w_state+=0.000000000001;\n",
    "            return Z_w_state;\n",
    "        else:\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2 = list(),list(),list(),list(),list(),list(),list(),list();\n",
    "            K = list();\n",
    "            for i in range(len(data)):\n",
    "                sample1 = data[i][0];\n",
    "                sample2 = data[i][1];\n",
    "                state1.append(sample1[0]);\n",
    "                #print(sample1);\n",
    "                w_state1.append(weight_obj(sample1[0]));\n",
    "                w_next_state1.append(weight_obj(sample1[2]));\n",
    "                state2.append(sample2[0]);\n",
    "                w_state2.append(weight_obj(sample2[0]));\n",
    "                w_next_state2.append(weight_obj(sample2[2]));\n",
    "                #beta1.append(self.target_policy[sample1[0],sample1[1]]/self.behaviour_policy[sample1[0],sample1[1]]);\n",
    "                beta1.append(self.beta[self.selected_policy,sample1[0],sample1[1]]);\n",
    "                #beta2.append(self.target_policy[sample2[0],sample2[1]]/self.behaviour_policy[sample2[0],sample2[1]]);\n",
    "                beta2.append(self.beta[self.selected_policy,sample2[0],sample2[1]])\n",
    "                K.append(sample1[2]==sample2[2]);\n",
    "            return (state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K);\n",
    "    \n",
    "  def get_state_distribution_ratio(self,selected_policy,run,t):\n",
    "        batch = self.get_batch();\n",
    "        eps = 0.04;\n",
    "        self.data_used[run] =self.data_used[run]+batch;\n",
    "        self.selected_policy = selected_policy\n",
    "        pairs = list(product(batch,repeat=2));\n",
    "        self.loss_episode = [];\n",
    "        for _ in range(100):\n",
    "            batch = self.get_batch();\n",
    "            state1,state2,w_state1,w_state2,w_next_state1,w_next_state2,beta1,beta2,K = self.get_w(pairs, self.weight_obj, len(batch));\n",
    "            Z_w_state = self.get_w(batch, self.weight_obj, len(batch),1);\n",
    "            self.w_loss = 0\n",
    "            for i in range(len(state1)):\n",
    "                self.w_loss+=(beta1[i]*(w_state1[i]/Z_w_state) - (w_next_state1[i]/Z_w_state))*(beta2[i]*(w_state2[i]/Z_w_state)-(w_next_state2[i]/Z_w_state))*K[i];\n",
    "            self.w_loss/=(2*self.batch_size);\n",
    "            self.optimizerW.zero_grad();\n",
    "            self.w_loss.backward();\n",
    "            self.optimizerW.step();\n",
    "            self.optimizerW.zero_grad();\n",
    "            self.Z.append(Z_w_state)\n",
    "        self.loss.append(self.w_loss.cpu().detach().numpy()[0]);\n",
    "        state_dist=[];\n",
    "        for i in range(self.nS):\n",
    "            w_state = self.weight_obj(i);\n",
    "            w_state = w_state.cpu().detach().numpy()[0];\n",
    "            state_dist.append(w_state);\n",
    "        return np.array(state_dist);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7845c926",
   "metadata": {},
   "source": [
    "Instead of sampling state, action, next_state values on the go, we do it before hand and store it in a list named 'data'. So, that when required we can simply pass this data and save some time by before hand sampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "470ab1b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_episode(T,state,behaviour_policy,P,batch_size):\n",
    "  #global P,behaviour_policy,batch_size;\n",
    "  data={};temp=[];\n",
    "  for t in range(1,T+1):\n",
    "    action = np.argmax(np.random.multinomial(1,behaviour_policy[state,:]))\n",
    "    next_state = np.argmax(np.random.multinomial(1,P[action,state,:]));\n",
    "    state = next_state;\n",
    "    temp.append([state,action,next_state]);\n",
    "    if(t%batch_size==0):\n",
    "      data[int(t/batch_size)-1]=temp[:];\n",
    "  return data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210907c",
   "metadata": {},
   "source": [
    "For our help we create a softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bdd3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(theta):\n",
    "  theta = np.exp(theta);\n",
    "  sum = np.sum(theta)\n",
    "  return theta/sum;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3363cb",
   "metadata": {},
   "source": [
    "We create a function naemd as preprocessing() which is responsible to perform the theta update. After every val=50 times, our learning rate is divided by 10. So if we start with lr = 1. After 50 instances, lr = 0.1. After 100 steps lr = 0.01 and so on...\n",
    "Now at each instant, we found the softmax of the theta values. Now from the given probability values we sample a policy that will be updated next. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7172c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing(nPOL,run,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val,val2,behaviour_policy_state_distribution,beta,batch_size,alpha):\n",
    "    lr = 1\n",
    "    nS = nPOL;\n",
    "    theta = np.ones(nPOL);\n",
    "    c=0;\n",
    "    rew = np.zeros(nPOL);\n",
    "    S = np.ones(nPOL);\n",
    "    F = np.ones(nPOL);\n",
    "    n = np.ones(nPOL);\n",
    "    for t in tqdm(range(1,T_update+1)):\n",
    "      sampled_policy = np.argmin([np.random.beta(S[j],F[j]) for j in range(nPOL)])\n",
    "      #policy_selected[t,run]=selected_policy;\n",
    "      policy_sampled[t-1,run] = sampled_policy;\n",
    "      #loss[t,run] = w_obj.w_loss.cpu().detach().numpy()[0];\n",
    "      w_obj.set_batch(data[t-1]);\n",
    "      sd = w_obj.get_state_distribution_ratio(sampled_policy,run,t-1);\n",
    "      sd = sd * behaviour_policy_state_distribution;\n",
    "      sd = sd/np.sum(sd)\n",
    "      rho_i = sum([sd[s]*R[target_policy[sampled_policy,s],s] for s in range(nS)]);\n",
    "      rew[sampled_policy] = (1-alpha)*rew[sampled_policy] + rho_i\n",
    "      S[sampled_policy]+=rew[sampled_policy];\n",
    "      F[sampled_policy]=F[sampled_policy]+t-rew[sampled_policy]\n",
    "      estimated_value[t-1,run] = rho_i;\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64129192",
   "metadata": {},
   "source": [
    "Finally call the hyperparameters, create the environment and call the pre-processing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82b9ad89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 2\n",
      "[[0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]\n",
      " [0.5 0.5]] [[[0.1        0.2        0.3        0.4       ]\n",
      "  [0.         0.22222222 0.33333333 0.44444444]\n",
      "  [0.         0.         0.42857143 0.57142857]\n",
      "  [0.         0.         0.         1.        ]]\n",
      "\n",
      " [[1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.        ]\n",
      "  [1.         0.         0.         0.        ]]]\n",
      "[0 0 0 1]\n",
      "[0 0 1 1]\n",
      "[0 1 1 1]\n",
      "[1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 1/2 [00:43<00:43, 43.69s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████| 2/2 [03:43<00:00, 111.86s/it]\u001b[A\n",
      " 20%|████████▊                                   | 1/5 [03:43<14:54, 223.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [[2, 0, 2], [0, 1, 0], [2, 0, 2], [0, 1, 0], [1, 0, 1], [0, 1, 0], [3, 0, 3], [3, 0, 3], [3, 0, 3], [3, 0, 3], [3, 0, 3], [0, 1, 0], [0, 1, 0], [2, 0, 2], [3, 0, 3], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [3, 0, 3], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]], 1: [[2, 0, 2], [0, 1, 0], [2, 0, 2], [0, 1, 0], [1, 0, 1], [0, 1, 0], [3, 0, 3], [3, 0, 3], [3, 0, 3], [3, 0, 3], [3, 0, 3], [0, 1, 0], [0, 1, 0], [2, 0, 2], [3, 0, 3], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [3, 0, 3], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [1, 0, 1], [0, 1, 0], [3, 0, 3], [3, 0, 3], [0, 1, 0], [2, 0, 2], [3, 0, 3], [0, 1, 0], [3, 0, 3], [0, 1, 0], [2, 0, 2], [2, 0, 2], [3, 0, 3], [3, 0, 3], [0, 1, 0], [1, 0, 1], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0], [2, 0, 2], [0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0]]}\n",
      "One run completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 1/2 [01:36<01:36, 96.42s/it]\u001b[A\n",
      " 20%|████████▊                                   | 1/5 [05:20<21:20, 320.16s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_998777/3946715021.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m#######################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnPOL\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_sampled\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpolicy_selected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mestimated_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mT_update\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbehaviour_policy_state_distribution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"One run completed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;31m#######################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_998777/2600769572.py\u001b[0m in \u001b[0;36mprocessing\u001b[0;34m(nPOL, run, policy_sampled, policy_selected, loss, estimated_value, T_update, P, R, val, val2, behaviour_policy_state_distribution, beta, batch_size, alpha)\u001b[0m\n\u001b[1;32m     14\u001b[0m       \u001b[0;31m#loss[t,run] = w_obj.w_loss.cpu().detach().numpy()[0];\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0mw_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m       \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_state_distribution_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampled_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m       \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbehaviour_policy_state_distribution\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0msd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_998777/5303110.py\u001b[0m in \u001b[0;36mget_state_distribution_ratio\u001b[0;34m(self, selected_policy, run, t)\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_state1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mZ_w_state\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw_next_state1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mZ_w_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_state2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mZ_w_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw_next_state2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mZ_w_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_loss\u001b[0m\u001b[0;34m/=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizerW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "    T,runs,lr,batch_size,state,nS,nA,rep_cost,alpha,gamma = get_hyperparameters().ret_hyperparameters();\n",
    "    print(nS,nA);\n",
    "    nPOL = nS;\n",
    "    T_update = int(T/batch_size);\n",
    "    mr_obj = Machine_Replacement(rep_cost,nS,nA);\n",
    "    P,R = mr_obj.gen_probability(),mr_obj.gen_expected_reward()\n",
    "    theta = np.ones(nPOL);\n",
    "    behaviour_policy = np.ones((nS,nA))*0.5;\n",
    "    print(behaviour_policy,P);\n",
    "    behaviour_policy_state_distribution = beh_pol_sd(P,behaviour_policy,nS,nA).state_distribution_simulated(0);\n",
    "    data = simulate_episode(T,state,behaviour_policy,P,batch_size);\n",
    "    with open('data_used','wb') as f:\n",
    "      pickle.dump(data,f);\n",
    "    target_policy = np.ones((nPOL,nS),dtype = np.int8)\n",
    "    #data_dict={0:[],1:[],2:[],3:[],4:[]};\n",
    "    val2 = []\n",
    "    for i in range(nPOL-1,0,-1):\n",
    "        target_policy[nPOL-i-1][0:i] = 0;\n",
    "    for t in target_policy:\n",
    "        val2.append(sum([R[t[s],s] for s in range(nS)]))\n",
    "    val2 = np.array(val2);\n",
    "    one_hot_target_policy = one_hot(target_policy,nS,nA)\n",
    "    w_obj = average_case_version_2(nS,nA,behaviour_policy,state,lr,one_hot_target_policy,batch_size,data_dict);\n",
    "    '''for i in range(nPOL):\n",
    "      w_obj[i].set_target_policy(one_hot_target_policy[i]);'''\n",
    "    policy_selected = np.zeros((T_update,runs))\n",
    "    policy_sampled = np.zeros((T_update,runs))\n",
    "    theta_change = []\n",
    "    estimated_value = np.zeros((T_update,runs))\n",
    "    loss = np.zeros((T_update,runs))\n",
    "    val = 50;\n",
    "    lr = 1;\n",
    "    beta = 1;\n",
    "    #######################################################################################\n",
    "    '''mp.set_start_method('spawn')\n",
    "    p1 = mp.Process(target = processing ,args=(nPOL,0,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val))\n",
    "    p2 = mp.Process(target = processing ,args=(nPOL,1,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val))\n",
    "    p3 = mp.Process(target = processing ,args=(nPOL,2,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val))\n",
    "    p4 = mp.Process(target = processing ,args=(nPOL,3,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val))\n",
    "    p5 = mp.Process(target = processing ,args=(nPOL,4,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val))\n",
    "    #p1 = mp.Process(target = processing ,args=(0,policy_sampled,policy_selected,loss,estimated_value,T_update,val))\n",
    "    #p1 = mp.Process(target = processing ,args=(0,policy_sampled,policy_selected,loss,estimated_value,T_update,val))\n",
    "    #p1 = mp.Process(target = processing ,args=(0,policy_sampled,policy_selected,loss,estimated_value,T_update,val))\n",
    "    #p1 = mp.Process(target = processing ,args=(0,policy_sampled,policy_selected,loss,estimated_value,T_update,val))\n",
    "    #p1 = mp.Process(target = processing ,args=(0,policy_sampled,policy_selected,loss,estimated_value,T_update,val))\n",
    "    p1.start();\n",
    "    p2.start();\n",
    "    p3.start();\n",
    "    p4.start();\n",
    "    p5.start();\n",
    "    p1.join();\n",
    "    p2.join();\n",
    "    p3.join();\n",
    "    p4.join();\n",
    "    p5.join();'''\n",
    "    #######################################################################################\n",
    "    for run in tqdm(range(runs)):\n",
    "        processing(nPOL,run,policy_sampled,policy_selected,loss,estimated_value,T_update,P,R,val,val2,behaviour_policy_state_distribution,beta,batch_size,alpha)\n",
    "        print(\"One run completed\");\n",
    "    #######################################################################################\n",
    "    #pd.DataFrame(policy_selected).to_excel(\"Policy_selection.xlsx\");\n",
    "    #pd.DataFrame(np.array(theta_change)).to_excel(\"Theta_values_non_estimate.xlsx\");\n",
    "    pd.DataFrame(policy_sampled).to_excel(\"Policy_sampling_Thompson_Sampling_variant_2.xlsx\");\n",
    "    pd.DataFrame(data).to_excel(\"Data_Used_Thompson_Sampling_variant_2.xlsx\");\n",
    "    pd.DataFrame(estimated_value).to_excel(\"Estimated_Value_functions_Thompson_Sampling_variant_2.xlsx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a16ec4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
